<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Benchmarking Visual Localization for Autonomous Navigation | Lauri Suomela</title> <meta name="author" content="Lauri Suomela"> <meta name="description" content=""> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A5%A6&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://lasuomela.github.io/carla_vloc_benchmark/"> </head> <body class=" sticky-bottom-footer"> <div class="container mt-5"> <div class="post"> <header class="post-header"> <center> <p class="post-description"> </p> <h1 class="post-title">Benchmarking Visual Localization for Autonomous Navigation</h1> <h3 class="project-venue" style="color:rgb(136, 136, 136);">WACV 2023<br><br> </h3> <h5 class="project-authors"> <a href="https://lasuomela.github.io">Lauri Suomela</a> <sup>1</sup> · <a href="https://www.linkedin.com/in/jussi-kalliola-0b0b71112" rel="external nofollow noopener" target="_blank">Jussi Kalliola</a> <sup>1</sup> · <a href="https://www.linkedin.com/in/atakandag/" rel="external nofollow noopener" target="_blank">Atakan Dag</a> <sup>1</sup> <br><br> <a href="https://www.linkedin.com/in/harryedelman/" rel="external nofollow noopener" target="_blank">Harry Edelman</a> <sup>2</sup> · <a href="https://scholar.google.fi/citations?user=r6Y4nacAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Joni-Kristian Kämäräinen</a> <sup>1</sup> <br><br> <em> <sup>1</sup> Tampere University · <sup>2</sup> Turku University of Applied Sciences </em> <br><br> </h5> <div class="container" style=" display:flex;flex-wrap:wrap;justify-content:center;"> <a href="https://arxiv.org/abs/2203.13048" class="project" rel="external nofollow noopener" target="_blank"> <div class="block" style="width: 100px;"> <i class="ai ai-arxiv ai-2x"></i><br> Arxiv </div> </a> <a href="https://github.com/lasuomela/carla_vloc_benchmark" class="project" rel="external nofollow noopener" target="_blank"> <div class="block" style="width: 100px;"> <i class="fab fa-github fa-2x"></i><br> Code </div> </a> <a href="/assets/pdf/carla_vloc_supplementary.pdf" class="project"> <div class="block" style="width: 100px;"> <i class="fa fa-file-pdf fa-2x"></i><br> Supplementary </div> </a> </div> </center> <hr> </header> <article> <div class="row"> <div class="container lazy" data-lazy-placeholder="https://placehold.it/1321x583?text=Loading" data-lazy-error="https://placehold.it/1321x583?text=Error"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/carla_vloc_benchmark/carla.webp" sizes="95vw"></source> <img src="/assets/img/carla_vloc_benchmark/carla.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Navigation with visual localization" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </div> <p><br></p> <p>This work introduces a simulator-based benchmark for visual localization in the autonomous navigation context. The dynamic benchmark enables investigation of how variables such as the time of day, weather, and camera perspective affect the navigation performance of autonomous agents that utilize visual localization for closed-loop control. Our experiments study the effects of four such variables by evaluating state-of-the-art visual localization methods as part of the motion planning module of an autonomous navigation stack. The results show major variation in the suitability of the different methods for vision-based navigation. To the authors’ best knowledge, the proposed benchmark is the first to study modern visual localization methods as part of a complete navigation stack.</p> <p><br></p> <h2 id="benchmark">Benchmark</h2> <p>Visual localization is an active research topic in computer vision, but usually the localization methods are evaluated using static datasets and it is unclear how well the methods work when the visual localization output is used for closed-loop control. As a solution we present a benchmark which enables easy experimentation with different visual localization methods as part of a navigation stack. The platform enables investigating various factors that affect visual localization and subsequent navigation performance. The benchmark is based on the Carla autonomous driving simulator and our <a href="https://github.com/lasuomela/visual_robot_localization" rel="external nofollow noopener" target="_blank">ROS2 port</a> of the <a href="https://github.com/cvg/Hierarchical-Localization" rel="external nofollow noopener" target="_blank">Hloc</a> visual localization toolbox.</p> <div class="video-wrap"> <div class="video-container"> <iframe src="https://www.youtube.com/embed/bAW3nfqSh2Q" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe> </div> </div> <p><br> <br></p> <h2 id="experiments">Experiments</h2> <p>We conducted experiments to determine how well different hierarchical visual localization methods cope with changes in gallery-to-query illumination, viewpoint and weather change. <br></p> <div class="video-wrap"> <div class="video-container"> <iframe src="https://www.youtube.com/embed/qgFp68cqqd8" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe> </div> </div> <p><br> <br></p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/carla_vloc_benchmark/experiment_conditions-480.webp 480w,/assets/img/carla_vloc_benchmark/experiment_conditions-800.webp 800w,/assets/img/carla_vloc_benchmark/experiment_conditions-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/carla_vloc_benchmark/experiment_conditions.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Experiment conditions" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Examples of illumination (a-c), viewpoint (d-f), and weather changes (g-i). </div> <p><br></p> <h2 id="results">Results</h2> <p><br> Quantitative results on two metrics: localization recall rate and navigation failure rate.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/carla_vloc_benchmark/failure_rate_figs-480.webp 480w,/assets/img/carla_vloc_benchmark/failure_rate_figs-800.webp 800w,/assets/img/carla_vloc_benchmark/failure_rate_figs-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/carla_vloc_benchmark/failure_rate_figs.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Failure rates" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Relationship of failure rate with illumination (a, b) and viewpoint change (c). Marker color indicates type for local features, shape for global features. </div> <p>Combinations using SuperPoint achieve the lowest failure rates, and by a clear margin. The best performing combination is that of SuperPoint and NetVLAD, followed by SuperPoint with Ap-GeM. This follows a general pattern: The local feature method seems to have more effect on the performance than the place recognition method. Of the two place recognition methods, NetVLAD provides a slightly better performance.</p> <p><br></p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/carla_vloc_benchmark/recall_rate_table-480.webp 480w,/assets/img/carla_vloc_benchmark/recall_rate_table-800.webp 800w,/assets/img/carla_vloc_benchmark/recall_rate_table-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/carla_vloc_benchmark/recall_rate_table.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Recall rates" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> The illumination experiment localization recall rates for the reference paths with thresholds T1 (≤0.25m, ≤2◦), T2 (≤0.50m,≤5◦) and T3 (≤5.00m, ≤10◦). Table with all values of k in the supplementary material. </div> <p><br></p> <center> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/carla_vloc_benchmark/failurerate_recall_correlation_Town01_accuracy_0.25m_2deg-480.webp 480w,/assets/img/carla_vloc_benchmark/failurerate_recall_correlation_Town01_accuracy_0.25m_2deg-800.webp 800w,/assets/img/carla_vloc_benchmark/failurerate_recall_correlation_Town01_accuracy_0.25m_2deg-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/carla_vloc_benchmark/failurerate_recall_correlation_Town01_accuracy_0.25m_2deg.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Rate correlation" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Relationship between the failure rate and recall rate T1 for the illumination experiments. Marker color and shape indicate feature type. See the supplementary material for the full set of figures. </div> </center> <p>SuperPoint, that achieves the lowest failure rates in different illumination conditions, also achieves the lowest failure rate for a given recall rate. There is a certain operation point, determined by odometry drift, after which changes in the recall rate become meaningless for autonomous navigation. Especially the second finding is interesting as it shows that visual localization performance needs to be sufficiently good in order to improve over wheel odometry only. For Town01, the recall rate of a method at threshold T1 has to be above 60% to benefit navigation. In other words, improving recall from 40% to 50% is almost meaningless while improvement from 60% to 70% is clearly significant.</p> <p><br></p> <h3 id="bibtex">BibTex</h3> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@InProceedings{Suomela_2023_WACV,
author    = {Suomela, Lauri and Kalliola, Jussi and Dag, Atakan and Edelman, Harry and Kämäräinen, Joni-Kristian},
title     = {Benchmarking Visual Localization for Autonomous Navigation},
booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
month     = {January},
year      = {2023},
pages     = {2945-2955}
}
</code></pre></div></div> </article> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container"> © Copyright 2026 Lauri Suomela. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams",inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]]}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-1NFYQKJB85"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-1NFYQKJB85");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>